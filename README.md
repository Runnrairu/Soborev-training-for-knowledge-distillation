# Soborev-training-for-knowledge-distillation
# ソボレフ正則化を用いたニューラルネットワークの性能向上

## はじめに

本実験では、ソボレフ正則化を用いてニューラルネットワーク蒸留の性能向上を目指す。特に、`gradients loss`を導入することで、出力の精度を改善するだけでなく、モデルの一般化能力を向上させることを目指した。以下に、この方法の概要、結果、および考察をまとめる。


---

## モデルの概要

### 1. **ターゲットモデル** (`f`)
- **入力**: 10次元のデータ
- **隠れ層**: 2つの512ユニットの隠れ層
- **出力**: 1つのスカラー値（回帰タスク）

### 2. **小型モデル** (`g`)
- **入力**: 10次元のデータ
- **隠れ層**: 1つの64ユニットの隠れ層
- **出力**: 1つのスカラー値（回帰タスク）

### 3. **目的**
- `f` は大規模なネットワークで、十分に訓練されたモデル。
- `g` はより小型のネットワークで、`f` と同等の性能を維持しながら学習することが目標。
- `g` の性能向上を目指し、ソボレフ正則化を導入して、`gradients loss`（勾配の差異）を損失関数に取り入れる。

---

## ソボレフ正則化の導入

### 1. **出力と勾配の差異**
- ソボレフ正則化を使う主な理由は、`f` と `g` の出力だけでなく、両者の勾配（導関数）の差異にも注目すること。これによりtrain_data近傍の挙動をとがらせないようにする。
- 出力の差異 (`output_difference`) と勾配の差異 (`gradient_difference`) をそれぞれ計算し、損失関数として組み込む。

### 2. **損失関数**
損失関数は以下の形で定義される。

$$
\text{Loss} := \text{OutputDifference} + \alpha \times \text{GradientDifference}
$$

- `OutputDifference` は `f` と `g` の出力の差を計算。
- `GradientDifference` は `grad f` と `grad g` の差を計算。
- `\alpha`は重みで、勾配の差異に対する優先度を調整する。特徴量の次元が10のとき、ここは5くらいがいいらしい。

---

## 結果

### 1. **テストエラーの比較**
- ソボレフ正則化を導入した場合、`g` のテストエラーは通常学習よりも大幅に改善された。
- 特に、`gradients loss` を加えた場合、`f` と `g` の出力だけでなく、`g` の一般化能力が向上した。

#### ソボレフ正則化あり:
- **fモデルのMSE**: 252
- **gモデルのMSE**: 374

#### 通常学習:
- **fモデルのMSE**: 280
- **gモデルのMSE**: 959

### 2. **出力と勾配の比較**
- `gradients loss` を加えると、`f` と `g` の出力の差異が小さくなり、また、勾配の差異も適切に反映されている。
- `g` のネットワークが `f` の性能に近づく結果が得られ、最終的な精度はソボレフ正則化を使うことで向上した。
  
---

## 考察

### 1. **勾配を加える意味**
- 単に `f(X) - g(X)` だけの損失を使うよりも、`grad f(X) - grad g(X)` を考慮することで、`g` が `f` の局所的な挙動をより良く学習する。
- 導関数の差を最小化することで、単なる出力の差を減らすだけでなく、`g` がより滑らかで安定した予測を行えるようになった。

### 2. **タスクの複雑さと効果**
- この手法は、特にタスクが単純な場合に効果を発揮しにくいが、モデルの表現能力に差がある場合や、モデルが非常に小さくなっている場合には有効正が高いと思われる。
- より複雑なタスクや大きなモデルにおいて、このアプローチはさらに効果的に機能する可能性がある。
- 学習済み大規模言語モデルの縮小版の製作などに希望が持てる。

---

## 今後の展望

- ソボレフ正則化は、特にニューラルネットの縮小問題において有用なアプローチであり、大規模言語モデル以外の問題にも応用できる可能性がある。

